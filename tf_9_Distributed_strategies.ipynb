{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf distributed strategies.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0EYFEXXolbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ways of training single model on multiple GPUs, computers or servers - to decrease training time.\n",
        "# 2 main categories of distributed strategies :\n",
        "# 1. Synchronous - generally train models on different parts of the dataset at the same time and at the end of one epoch, those gradients are aggregated and used to update one \n",
        "# model\n",
        "# 2. Asynchronous - all workers train at the same time independently and update weights asynchronously."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qf3GmGhuVM-",
        "colab_type": "text"
      },
      "source": [
        "COMMON DISTRIBUTED STRATEGIES IN TF 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th758IiLuUYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. MIRRORED STRATEGY - we start by defining our model on a single computer (here, will define on google colab). This strategy is used when we have multiple devices on a single\n",
        "# computer(multiple GPUs or CPUs) and it leverages all of them by creating a copy of the same model on each device. Each would train independently and at the end of one epoch, \n",
        "# it will update the main copy of the model. The aggregation of weights in most cases is done on the CPU or on the specific dedicated GPU for that. But sometimes, we're going to\n",
        "# have multiple servers or computers where each could have multiple devices on its own. How to scale the strategy to that level? (next strategy for that)\n",
        "\n",
        "# 2. MULTI WORKERS MIRRORED STRATEGY - In this case, we still have a primary model. But, it is not replicated for each device, but for each computer in the network, where each\n",
        "# machine is called a worker. Each worker could have multiple devices, in which case it works the same way that mirrored strategy worked. At the end of 1 epoch, we consider \n",
        "# all versions of the model to update the primary one. \n",
        "\n",
        "# 3. TPU STRATEGY - primarily used for GCP(google cloud platform) because they have TPUs on their servers.\n",
        "\n",
        "# 4. PARAMETER SERVER STRATEGY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyAUEKwkxQ3j",
        "colab_type": "text"
      },
      "source": [
        "SETUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpzfyoaoxMwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. select GPU as hardware accelerator\n",
        "# 2. install tf 2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vwsRSYlxhYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0.alpha0 # gpu version of tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7yKgiOjx8VU",
        "colab_type": "text"
      },
      "source": [
        "IMPORT PROJECT DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSc5nBDox7tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc0UcAPKxoS1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ec2fa98-8ad0-4fda-f3ec-46187d3b695d"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-alpha0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ7Y-oGsyiJI",
        "colab_type": "text"
      },
      "source": [
        "DATASET PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFp0xOySyf2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b88e88e2-b984-43d4-b462-9475a8ed1c06"
      },
      "source": [
        "# load mnist dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZBPgEI5yx-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image normalization - to accelerate training process (pixel ranges from 0-255 take longer to converge as opposed to from 0-1)\n",
        "X_train = X_train/255.0\n",
        "y_train = y_train/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaXm93XTzWeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ac37be5-fdce-4079-a72a-e93d644949ef"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZFS9FGizNgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset reshaping - we're using fully connected networks. Therefore, we need to reshape these images to be in vector size and not in matrix form.\n",
        "X_train = X_train.reshape(-1, 28*28)\n",
        "X_test = X_test.reshape(-1, 28*28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0g4-gfH0FIa",
        "colab_type": "text"
      },
      "source": [
        "# DEFINING A NON DISTIBUTED MODEL (NORMAL MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cskyxiqzzwzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_normal = tf.keras.Sequential()\n",
        "model_normal.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784,)))\n",
        "model_normal.add(tf.keras.layers.Dropout(rate=0.2))\n",
        "model_normal.add(tf.keras.layers.Dense(units=10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjv_8Dwt0zWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_normal.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7dsPy8c1IPG",
        "colab_type": "text"
      },
      "source": [
        "# DISTRIBUTED MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG9FEopE15_j",
        "colab_type": "text"
      },
      "source": [
        "Set up a distibuted strategy - Mirrored strategy used here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoDynv9B1Eh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "cee93f64-35ff-4465-dd66-9de08c9a077e"
      },
      "source": [
        "distribute = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd8KsXVV1-Z9",
        "colab_type": "text"
      },
      "source": [
        "Define a distributed model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXybGQG41fEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "ac3fca73-ccde-46a6-9b54-e67f8af362d7"
      },
      "source": [
        "with distribute.scope():\n",
        "  model_dis = tf.keras.Sequential()\n",
        "  model_dis.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784,)))\n",
        "  model_dis.add(tf.keras.layers.Dropout(rate=0.2))\n",
        "  model_dis.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
        "  model_dis.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unUvyvi-2mgi",
        "colab_type": "text"
      },
      "source": [
        "# SPEED TEST - NORMAL VS DISTRIBUTED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nklQIong2g5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "7647caae-74d6-4006-fb8a-06d90ba8a0a6"
      },
      "source": [
        "start_time = time.time()\n",
        "model_dis.fit(X_train, y_train, epochs=10, batch_size=25)\n",
        "print(\"Time taken by distributed training : {}\".format(time.time() - start_time))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:batch_all_reduce invoked for batches size = 4 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 12s 5ms/step - loss: 2.0780e-06 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 12s 5ms/step - loss: 3.8922e-07 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 12s 5ms/step - loss: 1.7388e-07 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.7000e-08 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 1.0669e-08 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 12s 5ms/step - loss: 6.7490e-09 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 5.9380e-09 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 12s 5ms/step - loss: 2.7815e-11 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.7815e-11 - sparse_categorical_accuracy: 0.0987\n",
            "Time taken by distributed training : 129.4754638671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMmVXGE33Gbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "34fdfa69-9c88-4045-e689-ae036a431e1c"
      },
      "source": [
        "start_time = time.time()\n",
        "model_normal.fit(X_train, y_train, epochs=10, batch_size=25)\n",
        "print(\"Time taken by normal training : {}\".format(time.time() - start_time))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0034 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 11s 177us/sample - loss: 1.6475e-06 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 11s 176us/sample - loss: 4.6261e-07 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 11s 178us/sample - loss: 1.2639e-07 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 4.6838e-08 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 2.1778e-08 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 173us/sample - loss: 3.4650e-09 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 11s 188us/sample - loss: 8.5560e-09 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 11s 181us/sample - loss: 6.6558e-10 - sparse_categorical_accuracy: 0.0987\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 171us/sample - loss: 8.0664e-10 - sparse_categorical_accuracy: 0.0987\n",
            "Time taken by normal training : 132.22743201255798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxVbfHRb4TQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not much difference here in time taken.\n",
        "# if we have just 1 gpu and 1 cpu in the device pool, the cpu will slow down the gpu and distributed training will perform worse than normal training on gpu. But if we have 2\n",
        "# or kore gpus, distributed will be better."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}